# plan.txt — Detailed step-by-step plan to build the AI (Crop Recommendation + Fertilizer + Profitability) with SHAP explainability

> This document is the single-source-of-truth AI plan you asked for. Follow it step-by-step in Windsurfer IDE (or your preferred environment). Each numbered step is actionable; sub-steps include commands, file names, and expected artifacts.

---

## 0. Quick summary (one line)
Build three ML components — **crop recommender (classifier)**, **fertilizer recommender (classifier/lookup)**, and **profit estimator (regressor)** — produce explainable outputs (SHAP) and deliver model artifacts + inference function for backend handoff.

---

## 1. Project setup 
1. Create project folder and initialize repository:
   ```bash
git init
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install --upgrade pip
```
2. Create canonical folders (exact names matter for scripts):
   ```text
   crop_ai/
   ├── data/
   │   ├── raw/            # put original CSVs here
   │   └── processed/      # generated by preprocessing
   ├── src/
   │   ├── preprocess.py
   │   ├── features.py
   │   ├── train_crop.py
   │   ├── train_fertilizer.py
   │   ├── train_profit.py
   │   ├── explain.py
   │   └── predict.py
   ├── models/             # saved model artifacts (.pkl/.onnx)
   ├── notebooks/          # optional EDA
   ├── tests/
   ├── requirements.txt
   └── README.md
   ```
3. Add minimal `requirements.txt` now (you can pin versions later):
   ```
   pandas
   numpy
   scikit-learn
   xgboost
   shap
   joblib
   pyyaml
   matplotlib
   ```
4. Commit initial skeleton.

---

## 2. Data ingestion & discovery 
**Goal:** understand raw files and decide which columns to keep for each model.

1. Copy source CSVs into `data/raw/`:
   - `Crop_recommendation.csv` (main training CSV for crop model)
   - `Fertilizer.csv` (fertilizer recommendations / dosages / costs)
   - `Crops_and_livestock_products.csv` (market prices, yields if available)
2. Open each file in a spreadsheet or with a small script to list column names and sample rows. Create `data/raw/README.md` summarizing columns and units (important for handoff).
3. Choose canonical units and column names for the project. Example canonical features for crop model:
   - `N, P, K` (units: kg/ha or relative index; keep same units)
   - `temperature` (°C)
   - `humidity` (%)
   - `ph` (pH units)
   - `rainfall` (mm; define period: annual/seasonal/monthly — be explicit)
   - `region` (optional; string)
   - `label` (crop name)

**Output:** `data/raw/README.md` and checklist of columns to keep.

---

## 3. Preprocessing & cleaning (src/preprocess.py) 
**Goal:** produce reproducible cleaned datasets and artifacts (scalers, label encoders, feature list).

1. Implement `load_raw()` that reads CSVs from `data/raw/` and returns dataframes.
2. For each dataset implement cleaning steps:
   - Standardize column names (lowercase, underscores).
   - Drop irrelevant columns (IDs, notes).
   - Convert numeric strings to numeric types; coerce errors.
   - Trim whitespace in categorical columns.
3. Missing value strategy:
   - Numeric columns: impute median (store medians in artifact file `models/num_imputer.json`).
   - Categorical columns: fill `"unknown"`.
   - If >30% missing for a column, evaluate drop vs domain imputation.
4. Outlier handling:
   - Clip rainfall to [0, 5000] mm (adjust per context), temperature to [-10, 55]°C, pH to [3.5, 9].
   - Document clip thresholds in `src/preprocess.py` as constants.
5. Encode labels:
   - `LabelEncoder` for `label` (crop) and for `region` if used.
   - Save encoders using `joblib.dump()` into `models/label_encoders/`.
6. Scaling:
   - Fit `StandardScaler` on numeric features; save scaler as `models/scaler_v1.pkl`.
7. Create and save `data/processed/crop_data_cleaned.csv` with canonical feature order.

**Unit tests:** `tests/test_preprocess.py` checks that function raises descriptive errors when input columns missing, and that output column order matches `feature_list.yaml`.

---

## 4. Feature engineering (src/features.py)
**Goal:** add derived features that improve model signal without leaking the target.

1. Add domain-driven derived features:
   - `rainfall_mam` (March–May) and `rainfall_jjas` (June–Sept) if monthly data available.
   - `temp_range` = `max_temp - min_temp` (if monthly or daily available).
   - `npk_balance` = e.g., `N - (P+K)/2` (experimentally useful).
   - `soil_fertility_index` combining pH and organic carbon if available.
2. Keep feature engineering deterministic and save pipeline functions; reuse in predict.
3. Save `models/feature_list.yaml` that is the exact list and order of features used by models.

---

## 5. Baseline model training — Crop recommender (src/train_crop.py)
**Goal:** train a robust classifier with basic tuning and produce model artifact + evaluation report.

1. Baseline algorithm: **LightGBM** or **XGBoost** (if not available, RandomForest). LightGBM often faster/accurate for tabular.
2. Train pipeline:
   - Load processed CSV.
   - Split: train/test = 80/20 (stratify by crop label). If region/geo available, consider spatial CV.
   - Cross-validate with StratifiedKFold (k=5) to estimate generalization; log fold metrics.
3. Training hyperparameters (start point):
   - n_estimators: 500 (use early stopping)
   - learning_rate: 0.05
   - max_depth: 8
   - num_leaves: 31
4. Metrics to compute:
   - Accuracy, F1-macro, Top-3 accuracy (probability ranking)
   - Confusion matrix, per-class support
5. Save artifacts:
   - `models/crop_model_v1.pkl` (joblib)
   - `models/crop_eval_v1.json` (metrics & confusion matrix)
6. Save training logs and Git tag `v1`.

**Deliverable:** trained model and evaluation report.

---

## 6. Fertilizer recommender (src/train_fertilizer.py)
**Goal:** provide fertilizer recommendations; simplest approach: lookup table fallback + supervised model where data exists.

1. Preferred approach: **hybrid**
   - If `Fertilizer.csv` gives direct mapping (`crop` + `soil_condition` -> `fertilizer`), use a rule-based lookup.
   - Else train a classifier: input = soil features + crop, output = fertilizer category.
2. Training details (if ML used): DecisionTree or RandomForest with balanced class weights.
3. Save artifact `models/fertilizer_model_v1.pkl` and `models/fertilizer_lookup.json` (if lookup implemented).
4. Save `models/fertilizer_eval_v1.json`.

**Deliverable:** predict function that takes (soil features + crop) and returns fertilizer + dosage + cost estimate.

---

## 7. Profit estimator (src/train_profit.py)
**Goal:** estimate expected profit per unit area for the recommended crop using yield and price data.

1. Design choices:
   - Option A: build a regression model that directly predicts `expected_net_profit` given features (crop, soil, weather, fertilizer recommendations, region, area).
   - Option B (recommended): decouple into **yield predictor** + deterministic price & cost computation. This improves interpretability.

2. If Option B:
   - Train `yield_model` (regressor) to predict `yield_t_per_ha` or `quintal_per_acre`.
   - Keeping yield model: LightGBM regressor with early stopping.
   - Use `Crops_and_livestock_products.csv` or Agmarknet for `market_price` (modal price). Keep a daily/weekly price cache but for model training use historical modal price aggregated to matching period.
   - Costs: `cost_of_cultivation` from MOSPI or constructed from fertilizer + labor + irrigation tables.

3. Calculation function (deterministic):
   - `gross = expected_yield * market_price`
   - `investment = sum(seeds + fertilizer_cost + labor + irrigation + misc)`
   - `net_profit = gross - investment`
   - `roi_percent = net_profit / investment * 100`

4. Save artifacts:
   - `models/yield_model_v1.pkl` (if trained)
   - `models/profit_calc_config.json` (cost tables)

**Deliverable:** `predict_profit(features)` that returns expected_yield, p10/p50/p90 (if quantiles available), gross, investment, net, roi.

---

## 8. SHAP explainability (src/explain.py)
**Goal:** produce concise, human-friendly `why` for every prediction using SHAP.

1. Explainer setup:
   - For tree models use `shap.TreeExplainer(model)`.
   - Create explainer instance at service startup (expensive to initialize once for large models but OK offline).

2. For a single sample flow:
   - Preprocess sample → `X_single` (1×d vector matching `feature_list.yaml`).
   - `shap_values = explainer.shap_values(X_single)` (for multiclass, pick shap for predicted class).
   - Map SHAP array → `feature_contributions`: list of (feature, value, shap_abs)
   - Sort by absolute contribution and pick top 3.

3. Map numeric feature contribution → human sentence (templates). Examples:
   - `rainfall`: if shap>0 and value>threshold → "High rainfall (200 mm) — favors paddy/rice."
   - `ph`: categorize value (acidic/neutral/alkaline) and mention suitability.
   - `N/P/K`: if N low and shap negative → "Low Nitrogen reduces suitability; consider urea." 

4. Add a short generic caution: "This is a recommendation based on available data; local agronomist advice is recommended for final decisions."

5. Save small snapshots of explanation images for debugging in `models/explain_plots/` (optional).

**Deliverable:** function `explain_prediction(model, X_single)` returning `why: [str, str, str]` and `feature_imp: [{f, val, shap}]`.

---

## 9. Inference wrapper & API contract (src/predict.py)
**Goal:** provide a single callable function and a light CLI wrapper so backend can integrate easily.

1. Implement `predict_from_dict(input_dict) -> dict` that:
   - Validates input schema and units.
   - Applies preprocessing and feature engineering.
   - Calls `crop_model.predict_proba` and picks top suggestion (crop) + confidence.
   - Calls `fertilizer_predictor(crop, soil_features)` to get fertilizer + dosage + cost.
   - Calls `yield_predictor` or uses historical average to get expected_yield and interval.
   - Computes profit via profit calculator.
   - Calls `explain_prediction` to get top-3 reasons.
   - Returns JSON (see contract below).

2. CLI wrapper:
   ```bash
   python src/predict.py --input sample_input.json --output sample_output.json
   ```

3. Output JSON contract (stable):
   ```json
   {
     "recommended_crop": "rice",
     "confidence": 0.87,
     "why": ["High rainfall (200 mm) — favors Rice", "pH 6.5 — within Rice optimal range"],
     "expected_yield_t_per_acre": 2.5,
     "yield_interval_p10_p90": [2.0, 3.2],
     "profit_breakdown": {"gross": 55000, "investment": 19000, "net": 36000, "roi": 189},
     "model_version": "crop_model_v1"
   }
   ```

**Deliverable:** `src/predict.py` with `predict_from_dict` function and small CLI.

---

## 10. Testing & quality gates
1. Unit tests (`tests/`):
   - `test_preprocess.py`: missing columns raise descriptive errors.
   - `test_train.py`: small bootstrap train runs without errors.
   - `test_predict.py`: given sample input, keys exist and types are correct.
2. Smoke tests: run `predict_from_dict` on 50 random samples, ensure distribution of recommendations similar to training distribution (no single class collapse).
3. Manual review by domain expert: sample 100 predictions + explanations; get agronomist approval for phrasing.

---

## 11. Packaging & handoff
1. Final artifact folder to hand to backend:
   - `models/crop_model_v1.pkl`
   - `models/yield_model_v1.pkl` (if used)
   - `models/fertilizer_model_v1.pkl` or `fertilizer_lookup.json`
   - `models/scaler_v1.pkl`, `models/label_encoders/*.pkl`
   - `models/feature_list.yaml`, `models/profit_calc_config.json`
   - `src/predict.py` (predict_from_dict)
   - `src/explain.py` (explain_prediction)
   - `requirements.txt`
   - `model_card.md` (one-page: dataset, metrics, limitations, model version, expected latency)
2. Provide a short `HOW_TO_RUN.md` for backend devs showing CLI usage and sample curl to a local Python FastAPI wrapper if they choose to deploy Python service.

---

## 12. Monitoring & feedback (post-handoff)
1. Log predictions with `input + output + timestamp + user_feedback` to `data/feedback/` for retraining cycle.
2. Periodically retrain monthly/quarterly depending on feedback volume.
3. Monitor confidence metric; alert if mean confidence drops below threshold.

---

## 13. Timeline (recommended)
- Week 1: Data discovery, preprocessing, baseline crop model training.
- Week 2: Fertilizer pipeline, profit calculation, yield model baseline.
- Week 3: SHAP explainability integration, predict wrapper, unit tests.
- Week 4: Model polishing, documentation, handoff artifacts & model card.

---

## 14. Appendix: Quick-code pointers (small reminders)
- Use `joblib.dump(obj, path)` and `joblib.load(path)` for scikit-learn artifacts.
- For LightGBM, use early stopping via validation set to avoid overfit.
- Save random seed in `config.py` and use it everywhere (train/test split, CV).
- Keep `feature_list.yaml` authoritative and read it both in train and predict.
- For SHAP: compute `explainer = shap.TreeExplainer(model)` after model load and reuse it for multiple requests; store background dataset sample to initialize KernelExplainer if needed.

---

## 15. Final checklist before handoff
- [ ] Processed datasets saved in `data/processed/`
- [ ] All models saved in `models/` with version tags
- [ ] `src/predict.py` exposes `predict_from_dict()` and CLI
- [ ] `src/explain.py` returns top-3 human phrases
- [ ] `requirements.txt` complete
- [ ] `model_card.md` prepared and reviewed by agronomist

---

End of plan.txt. Follow these steps and update the repository as you progress. If you want, I can now generate the **predict.py** and **explain.py** boilerplate files (lightweight) to speed up development.

